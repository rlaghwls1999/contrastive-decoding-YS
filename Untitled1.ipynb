{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f42fe3-f003-40a3-b556-acad362c8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5fbab9c-1b70-45b3-a2f9-27965043e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models/tokenizers\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e71aaf28-bc9f-4be3-bbd0-babd9bb3689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "expert_model_name = \"gpt2-large\"\n",
    "amateur_model_name = \"gpt2-medium\"\n",
    "\n",
    "expert_tokenizer, expert_model = load_model_and_tokenizer(expert_model_name)\n",
    "amateur_tokenizer, amateur_model = load_model_and_tokenizer(amateur_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0346f139-413c-4a12-a6a4-6871bfef571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive decoding function\n",
    "def contrastive_decoding(prompt, expert_model, amateur_model, tokenizer, alpha=0.7, max_length=100):\n",
    "    \"\"\"\n",
    "    Perform contrastive decoding given expert/amateur models.\n",
    "\n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        expert_model: Pretrained expert LLM.\n",
    "        amateur_model: Pretrained amateur LLM.\n",
    "        tokenizer: Tokenizer for encoding/decoding.\n",
    "        alpha: Amateur penalty.\n",
    "        max_length: Maximum length of generated tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: Text generated using contrastive decoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    current_ids = input_ids.clone()\n",
    "    \n",
    "    # Move computation to gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    expert_model.to(device)\n",
    "    amateur_model.to(device)\n",
    "    current_ids = current_ids.to(device)\n",
    "\n",
    "    # Generate tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(max_length):\n",
    "        # Get logits from both models\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            expert_logits = expert_model(current_ids).logits[:, -1, :]  # Expert model logits\n",
    "            amateur_logits = amateur_model(current_ids).logits[:, -1, :]  # Amateur model logits\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        expert_probs = F.softmax(expert_logits, dim=-1)\n",
    "        amateur_probs = F.softmax(amateur_logits, dim=-1)\n",
    "\n",
    "        # Adjust expert probabilities using amateur probabilities\n",
    "        contrastive_probs = expert_probs - alpha * amateur_probs\n",
    "        contrastive_probs = F.softmax(contrastive_probs, dim=-1)  # Normalize again\n",
    "\n",
    "        # Sample next token\n",
    "        next_token = torch.argmax(contrastive_probs, dim=-1)\n",
    "\n",
    "        # Append token and prepare for next step\n",
    "        generated_tokens.append(next_token.item())\n",
    "        current_ids = torch.cat([current_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        # Stop if generated token is the end-of-sequence token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "#plausability constraint\n",
    "#generic code base --> contrastive decoding\n",
    "    # llama as expert\n",
    "    #huggingface alignment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b83707-beae-4428-8129-48a51c32cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to understand what the word \"code\" means.\n",
      "\n",
      "The word \"code\" means \"to put something together.\"\n",
      "\n",
      "The word \"code\" means \"to put something together.\"\n",
      "\n",
      "The word \"code\" means \"to put something\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "prompt = \"The first step to becoming good at coding is\"\n",
    "output_text = contrastive_decoding(\n",
    "    prompt,\n",
    "    expert_model,\n",
    "    amateur_model,\n",
    "    expert_tokenizer,\n",
    "    alpha=0.7,\n",
    "    max_length=50\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5dc4bb-1a90-4878-b89a-c406e23af694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
